---
title: "Audição Cognitiva"
author: "Jônatas Bertolazzo, Letticia Nicoli, Renato Ramos"
date: "22/10/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Análise de texto a partir de arquivos de audio transcritos
### utilizando a plataforma do Watson

Neste trabalho utilizamos a tecnologia "Speech to Text" presente no Watson para transcrever alguns arquivos de áudio que responderam a pergunta "Você acredita que a Inteligência Artificial e os robos irão reduzir as oportunidades de emprego, 'roubando' o emprego dos humanos?".

Com todos os audios transcritos passamos para a etapa de NLP para tentar projetar uma resposta que resuma todo o conteudo contido dentro dos audios.

```{r include=FALSE}
library(tm)
library(NLP)
library(qdap) # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data wrangling, pipe operator %>%().
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.

rm(list = ls())
ls()
getwd()
setwd("C:/Users/Wulfrick/Desktop/#alabasta/AIML6_AC")


cps <- Corpus(DirSource('C:/Users/Wulfrick/Desktop/#alabasta/AIML6_AC',
                        encoding = "UTF-8"),
              readerControl = list(language = "pt"))

```

Após carregar os arquivos, o primeiro passo que utilizamos é tratar a formatação do texto, retirando espaços em branco, pontuação e reduzindo tudo a caixa baixa.

```{r echo=TRUE}
cps <- tm_map(cps, stripWhitespace)
cps <- tm_map(cps, content_transformer(tolower))
cps <- tm_map(cps, removeNumbers)
```

Depois de fazer este primeiro tratamento podemos ver o estado em que o nosso texto e suas principais palavras se encontram, para isso utilizamos uma plotagem no estilo "Wordcloud".

```{r echo=TRUE, warning=FALSE}
tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))

m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]

library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
          random.order = F, colors = pal)
```

Podemos ver no gráfico que o nosso texto fica bastante sujeito a palavras que á principio não fazem sentido na nossa análise, como por exemplo a palavras 'de, que' e os artigos 'e, a'.

passamos então para a análise e retirada de Stopwords, começando com a biblioteca padrão de stopwords de palavras em português.

```{r echo=TRUE}
cps <- tm_map(cps, removeWords, stopwords("portuguese"))
```

Utilizamos do processo de TDM para a criação de uma matrix termos para que seja possível separar quais são os termos mais frequentes.

```{r echo=TRUE}
tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))
```

Com as palavras separadas, olhamos as palavras mais frequentes da nossa lista.

Neste caso estamos visulizando as palavras com uma frequencia maior do que 50.

```{r echo=TRUE}
(freq.terms <- findFreqTerms(tdm, lowfreq = 50))
```

Aqui reparamos que algumas palavras ainda não são tão importantes exclusivamente para a análise que queremos fazer e podemos retirá-las assim como fizemos com os Stopwords.

```{r echo=TRUE}
cps <- tm_map(cps, removeWords, c(
                                  #More stop words
                                  "wer", '-', '"', 'wav', 'flac', 'mp3', 'tão', 'é', 'né', "lá", "dois", "seis", "cinco", "boa", "a"))

tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))
```

Podemos repetir este processo, como já fizemos, várias vezes, a fim de ir moldando a nossa base para que seja possível retirar as respostas que queremos dela.

Agora que temos a base tratada da forma que queremos, vamos começar a nossa análise de texto para descobrir sobre o conteúdo relevante que todos esses textos contém.

Para começar olhamos algumas correlações os termos com algumas 'palavras-chaves', neste caso, as palavras importantes para nós por exemplo são: 'artificial', 'inteligência', 'trabalho' e 'humanos'. Escolhemos palavras com correlação maior do que 0.7.

```{r}
findAssocs(tdm, "artificial", 0.7)
findAssocs(tdm, "inteligência", 0.7)
findAssocs(tdm, "trabalho", 0.7)
findAssocs(tdm, "humanos", 0.7)
```

Novamente vamos utilizar o plot "Wordcloud" para visualizar a disposição das palavras dentro da nossa base e, possívelmente, retirar novos 'insights' que podem ajudar na nossa análise.

```{r echo=TRUE, warning=FALSE}
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]

library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
          random.order = F, colors = pal)
```

Neste momento estamos repetindo o processo de retirada e plotagem da base levando em configuração as informações que conseguimos através dos processos anteriores.

Vimos que ainda temos algumas palavras que podem ser retiradas, como por exemplo encontramos as palavras 'mp3', 'wav' e 'flac' que são os formatos dos audios que recebemos.

```{r echo=TRUE, warning=FALSE}
cps <- tm_map(cps, removeWords, c(
                                  #More stop words
                                  "wer", '-', '"', 'wav', 'flac', 'mp3', 'tão', 'é', 'né', "lá", "dois", "seis", "cinco", "boa", "vai", 'ta', 'gente','vinte', 'tá', 'cada', 'gol', 'kb', 'tati', 'bi', 'ma', 'mil', 'tal', 'kit', 'ex', 'd', 't', 'i', 'c', 'l', 'j', 's', 'li', 'es', 'mp', 'u', 'b', 'm', 'rua'))

tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))

m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]

library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
          random.order = F, colors = pal)
```

Tabém trabalhamos nesta parte com Clusterização e KMeans para dividir nossa base em grupos diferentes de palavras, mais uma vez, estas técnicas nos ajudar a retirar 'insights' para melhorar a nossa resposta.

```{r}
tdm2 <- removeSparseTerms(tdm, sparse = 0.90)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")

m3 <- t(m2)
k <- 6 
kmeansResult <- kmeans(m3, k)
round(kmeansResult$centers, digits = 3)

for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T)
cat(names(s)[1:10], "\n")
}
```

Com todos estes dados podemos enfim concluir a nossa análise.

Analizando todos os textos podemos formular um resumo geral sobre todo o conteúdo disponível:

"A inteligência Artificial vai roubar o emprego e os humanos vão adaptar a forma de trabalho"
