---
title: "Audição Cognitiva"
author: "Jônatas Bertolazzo, Letticia Nicoli, Renato Ramos"
date: "22/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Análise de texto a partir de arquivos de audio transcritos
### utilizando a plataforma do Watson

Neste trabalho utilizamos a tecnologia "Speech to Text" presente no Watson para transcrever alguns arquivos de áudio que responderam a pergunta "Você acredita que a Inteligência Artificial e os robos irão reduzir as oportunidades de emprego, 'roubando' o emprego dos humanos?".

Com todos os audios transcritos passamos para a etapa de NLP para tentar projetar uma resposta que resuma todo o conteudo contido dentro dos audios.

```{r include=FALSE}
library(tm)
library(NLP)
library(qdap) # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data wrangling, pipe operator %>%().
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.

rm(list = ls())
ls()
getwd()
setwd("E:/#alabasta/AIML6_AC")


cps <- Corpus(DirSource('E:/#alabasta/AIML6_AC',
                        encoding = "UTF-8"),
              readerControl = list(language = "pt"))

```

Após carregar os arquivos, o primeiro passo que utilizamos é tratar a formatação do texto, retirando espaços em branco, pontuação e reduzindo tudo a caixa baixa.

```{r echo=TRUE}
cps <- tm_map(cps, stripWhitespace)
cps <- tm_map(cps, content_transformer(tolower))
cps <- tm_map(cps, removeNumbers)
```

Depois de retirar algumas possíveis erros de escrita do nosso texto, passamos para a análise e retirada de Stopwords.

Para começar testamos a biblioteca padrão de stopwords.

```{r echo=TRUE}
cps <- tm_map(cps, removeWords, stopwords("portuguese"))
```

Utilizamos do processo de TDM para a criação de uma matrix "alguma coisa" e separar quais são os termos mais frequentes.

```{r echo=TRUE}
tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))
```

Com as palavras separadas, olhamos as palavras mais frequentes da nossa lista

```{r echo=TRUE}
(freq.terms <- findFreqTerms(tdm, lowfreq = 50))
```

Aqui reparamos que algumas palavras não são tão importantes para a nossa análise e podemos retirá-las assim como fizemos com os Stopwords.

```{r echo=TRUE}
cps <- tm_map(cps, removeWords, c(
                                  #More stop words
                                  "wer", '-', '"', 'wav', 'flac', 'mp3', 'tão', 'é', 'né', "lá", "dois", "seis", "cinco", "boa"))

tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))
```

Com a base tratada podemos começar a tirar as primeiras conclusões analisando os termos associados.

```{r}
findAssocs(tdm, "artificial", 0.7)
findAssocs(tdm, "inteligência", 0.7)
findAssocs(tdm, "trabalho", 0.7)
findAssocs(tdm, "humanos", 0.7)
```

Para uma visualização gráfica utilizamos um plot atravéz de "Word Cloud", facilitando a visualização das palavras mais frequentes.

```{r echo=TRUE, warning=FALSE}
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]

library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
          random.order = F, colors = pal)
```

Após essa visualização do gráfico e dos termos mais frequentes, voltamos para a fase de tratamento de dados e retiramos mais algumas palavras que ainda não estão fazendo sentido em nossa análise e refazemos todo o processo de plotagem.

```{r echo=TRUE, warning=FALSE}
cps <- tm_map(cps, removeWords, c(
                                  #More stop words
                                  "wer", '-', '"', 'wav', 'flac', 'mp3', 'tão', 'é', 'né', "lá", "dois", "seis", "cinco", "boa", "vai", 'ta', 'gente','vinte', 'tá', 'cada', 'gol', 'kb', 'tati', 'bi', 'ma', 'mil', 'tal', 'kit', 'ex', 'd', 't', 'i', 'c', 'l', 'j', 's', 'li', 'es', 'mp', 'u', 'b', 'm', 'rua'))

tdm <- TermDocumentMatrix(cps,
                          control = list(wordLengths = c(1, Inf)))

m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]

library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
          random.order = F, colors = pal)
```

Tabém podemos trabalhar com o processo de Clusterização para dividir os termos proximos em grupos distintos a fim de enriquecer a análise.

matrix transposta e kmeans

```{r}
tdm2 <- removeSparseTerms(tdm, sparse = 0.90)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")

m3 <- t(m2)
k <- 6 
kmeansResult <- kmeans(m3, k)
round(kmeansResult$centers, digits = 3)

for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T)
cat(names(s)[1:10], "\n")
}
```

